{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOulHJAQ4QtGwL+56z6Nrk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergiomar73/nlp-google-colab/blob/main/Paraphrase_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "HTFMqydyIwWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "6dJ2KvjsLhMi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.sbert.net/docs/pretrained_models.html\n",
        "\n",
        "Order by Performance Sentence Embeddings (14 Datasets) DESC\n",
        "\n",
        "```\n",
        "Model Name\t Performance Sentence Embeddings (14 Datasets) \tPerformance Semantic Search (6 Datasets) \tAvg. Performance \tSpeed \tModel Size \n",
        "sentence-t5-xxl \t          70.88\t54.40\t62.64\t50\t9230 MB\n",
        "gtr-t5-xxl                  70.73\t55.76\t63.25\t50\t9230 MB\n",
        "all-roberta-large-v1 \t      70.23\t53.05\t61.64\t800\t1360 MB\n",
        "all-mpnet-base-v1 \t        69.98\t54.69\t62.34\t2800\t420 MB\n",
        "gtr-t5-large \t              69.90\t54.85\t62.38\t800\t640 MB\n",
        "gtr-t5-xl \t                69.88\t55.88\t62.88\t230\t2370 MB\n",
        "all-mpnet-base-v2 \t        69.57\t57.02\t63.30\t2800\t420 MB\n",
        "sentence-t5-xl \t            69.23\t51.19\t60.21\t230\t2370 MB\n",
        "all-MiniLM-L12-v1 \t        68.83\t50.78\t59.80\t7500\t120 MB\n",
        "sentence-t5-large \t68.74\t49.05\t58.89\t800\t640 MB\n",
        "all-distilroberta-v1 \t68.73\t50.94\t59.84\t4000\t290 MB\n",
        "all-MiniLM-L12-v2 \t68.70\t50.82\t59.76\t7500\t120 MB\n",
        "all-MiniLM-L6-v2 \t68.06\t49.54\t58.80\t14200\t80 MB\n",
        "all-MiniLM-L6-v1 \t68.03\t48.07\t58.05\t14200\t80 MB\n",
        "paraphrase-mpnet-base-v2 \t67.97\t47.43\t57.70\t2800\t420 MB\n",
        "sentence-t5-base \t67.84\t44.63\t56.23\t2500\t210 MB\n",
        "gtr-t5-base \t67.65\t51.15\t59.40\t2500\t210 MB\n",
        "multi-qa-mpnet-base-dot-v1 \t66.76\t57.60\t62.18\t2800\t420 MB\n",
        "multi-qa-distilbert-dot-v1 \t66.67\t52.51\t59.59\t4000\t250 MB\n",
        "multi-qa-mpnet-base-cos-v1 \t66.29\t57.46\t61.88\t2800\t420 MB\n",
        "paraphrase-distilroberta-base-v2 \t66.27\t43.10\t54.69\t4000\t290 MB\n",
        "paraphrase-TinyBERT-L6-v2 \t66.19\t41.07\t53.63\t4500\t240 MB\n",
        "paraphrase-MiniLM-L12-v2 \t66.01\t43.01\t54.51\t7500\t120 MB\n",
        "multi-qa-distilbert-cos-v1 \t65.98\t52.83\t59.41\t4000\t250 MB\n",
        "paraphrase-multilingual-mpnet-base-v2 \t65.83\t41.68\t53.75\t2500\t970 MB\n",
        "paraphrase-MiniLM-L6-v2 \t64.82\t40.31\t52.56\t14200\t80 MB\n",
        "paraphrase-albert-small-v2 \t64.46\t40.04\t52.25\t5000\t43 MB\n",
        "multi-qa-MiniLM-L6-cos-v1 \t64.33\t51.83\t58.08\t14200\t80 MB\n",
        "paraphrase-multilingual-MiniLM-L12-v2 \t64.25\t39.19\t51.72\t7500\t420 MB\n",
        "multi-qa-MiniLM-L6-dot-v1 \t63.90\t49.19\t56.55\t14200\t80 MB\n",
        "msmarco-bert-base-dot-v5 \t62.68\t52.11\t57.39\t2800\t420 MB\n",
        "msmarco-distilbert-base-tas-b \t62.57\t49.25\t55.91\t4000\t250 MB\n",
        "paraphrase-MiniLM-L3-v2 \t62.29\t39.19\t50.74\t19000\t61 MB\n",
        "msmarco-distilbert-dot-v5 \t61.84\t49.47\t55.66\t4000\t250 MB\n",
        "distiluse-base-multilingual-cased-v1 \t61.30\t29.87\t45.59\t4000\t480 MB\n",
        "distiluse-base-multilingual-cased-v2 \t60.18\t27.35\t43.77\t4000\t480 MB\n",
        "average_word_embeddings_komninos \t51.13\t21.64\t36.39\t22000\t240 MB\n",
        "average_word_embeddings_glove.6B.300d \t49.79\t22.71\t36.25\t34000\t420 MB\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tKbBPARdPzoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('paraphrase-albert-small-v2')\n",
        "\n",
        "# all-roberta-large-v1             => ['1.00', '0.58', '0.47', '0.30', '-0.05']\n",
        "# all-mpnet-base-v1                => ['1.00', '0.62', '0.57', '0.38', ' 0.04']\n",
        "# all-mpnet-base-v2                => ['1.00', '0.59', '0.57', '0.42', ' 0.05']\n",
        "# gtr-t5-large                     => ['1.00', '0.82', '0.81', '0.72', ' 0.49']\n",
        "# all-distilroberta-v1             => ['1.00', '0.58', '0.55', '0.32', ' 0.08']\n",
        "# paraphrase-distilroberta-base-v2 => ['1.00', '0.74', '0.69', '0.39', '-0.03']\n",
        "# paraphrase-albert-small-v2       => ['1.00', '0.67', '0.52', '0.40', ' 0.03']"
      ],
      "metadata": {
        "id": "NS6LzxL6PqCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsplv8xPGM5E",
        "outputId": "48d3a07c-1441-4a06-97fc-64b230a60694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: ['1.00', '0.67', '0.52', '0.40', '0.03']\n"
          ]
        }
      ],
      "source": [
        "# Two lists of sentences\n",
        "sentences1 = [\n",
        "  'most advanced conversation intelligence and AI powered coaching platform',\n",
        "  'Oh, so the quantified platform is one of the most advanced communication intelligence in AI powered coaching systems.',\n",
        "  'Oh, so the quantified platform is one of the most advanced communication intelligence in AI powered coaching systems. And what does that really mean? So, um, communication coaching is something that is typically delivered one on one between a communication coach who has a, uh, a doctorate or a, um, background and experience in teaching people how to be better communicators and how to express themselves effectively.',  \n",
        "  'And what does that really mean? So, um, communication coaching is something that is typically delivered one on one between a communication coach who has a, uh, a doctorate or a, um, background and experience in teaching people how to be better communicators and how to express themselves effectively.',\n",
        "  'The new movie is awesome'\n",
        "]\n",
        "sentences2 = [\n",
        "  'most advanced conversation intelligence and AI powered coaching platform',\n",
        "  'most advanced conversation intelligence and AI powered coaching platform',\n",
        "  'most advanced conversation intelligence and AI powered coaching platform',\n",
        "  'most advanced conversation intelligence and AI powered coaching platform',\n",
        "  'most advanced conversation intelligence and AI powered coaching platform',\n",
        "]\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "#Output the pairs with their score\n",
        "#for i in range(len(sentences1)):\n",
        "#  print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i][:50], sentences2[i][:50], cosine_scores[i][i]))\n",
        "print(f\"Scores: { [ f'{cosine_scores[i][i]:.2f}' for i in range(len(sentences1)) ] }\")"
      ]
    }
  ]
}